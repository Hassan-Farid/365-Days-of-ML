{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04abd701",
   "metadata": {},
   "source": [
    "# Data Quantity Requirement Analysis:\n",
    "Let's start with one sub-step of Structural Investigation step of the EDA phase as I mentioned [here](https://github.com/Hassan-Farid/365-Days-of-ML/blob/main/Exploratory%20Data%20Analysis.ipynb). Now, in later stages before we move towards model development phase, we have to ensure that the amount of given data satisfies our needs i.e. the data is both versatile in quantity as well as quality for the given use case. I will leave the topic for quality when we will look into quality investigation phase of EDA, for now we will be focusing only on quantity analysis as it corresponds with the structural investigation phase of EDA. \n",
    "\n",
    "## Influence of Data Quantity on ML Model:\n",
    "Data is an essential need for training and developing a machine-learning model. It is true that the amount of data directly influences the performance of a machine learning model, but it depends on different factors as to how much data is needed to solve it. I will list down some of these factors as under:\n",
    "\n",
    "* Complexity of the Problem\n",
    "* Complexity of Learning Approach\n",
    "* Success Threshold Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784a459",
   "metadata": {},
   "source": [
    "### Complexity of the Problem:\n",
    "One factor that influences the need for data quantity is the complexity of the problem at hand. If the problem is relatively simple in nature, it may not need as much data as you would think. By simple, I mean that the number of records and features that the model needs to take into account for finding a successful pattern is low. A very popular example of such case is the well known Iris dataset. The dataset has only _150_ records and _5_ variables. Thus, to train a model on this dataset, one would need to use only _4_ features (1 is the target variable) which themselves are diverse in quality and thus are sufficient to train a good classification model. Now, if the problem was more complex in nature, this wouldn't have been the case. In a complex problem, the model needs to take into account many parameters for its prediction and thus needs more data to give a good result. An example for this can be found [here](https://www.kaggle.com/competitions/predict-potential-spammers-on-fiverr). The dataset was provided in a community competition to detect spammers on Fiver based on the set of _53_ features provided by them. You can guess that based on whatever different variables they gathered, we would need a high number of records to successfully arrive at a model that can find patterns from the provided features. This is visible from the fact that they have given a dataset with _458798_ records just in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540d338",
   "metadata": {},
   "source": [
    "### Complexity of Learning Approach:\n",
    "Another factor that influences the need for data quantity is the complexity of the learning approach being used. For every problem, we can always apply a variety of learning solutions to get a good result. The complexity of learning approach directly depends on how that approach deals with the data or the computation process it uses to get its results. Simpler machine learning alogrithms like linear regression, Naive Bayes, etc. make use of relatively smaller datasets as opposed to learning algorithms like GANs, BERT, etc. This is because deep learning models have more complex behavior in comparison to general machine learning models due to the high number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428a381",
   "metadata": {},
   "source": [
    "### Success Threshold Criteria:\n",
    "The success criteria is also an important factor in determining the quantity of data needed to train the model. This factor depends on the type of problem being handled. Lets say, we are dealing with a machine learning classification problem of identifying cars on a track. Although the model to train for this case must be efficient but we can put it into practice even if it gives a result of approximately 80-90%. However, if we take the same classification problem and use it for diagnosing some form of life-threatening disease in a patient, it would backfire even with 80-90% and thus we would need to be as efficient as we can possibly get to put it into practice. Since the efficieny of a model directly corresponds with the amount of data required, we can say success thresholds directly affect the quantity of data needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a460f3",
   "metadata": {},
   "source": [
    "## Measuring Data Quantity:\n",
    "A thing to keep in mind is that the number of records provided to us in the EDA phase do not represent the data quantity of the dataset for the problem under consideration, rather the data obtained after preprocessing step gives us the insights on its quantity. Collected Data sent to Data Scientists for EDA and preprocessing sometimes contain data which can be looked into from different perspectives based on the type of problem being dealt with. Lets say we get a collected dataset for a job survey with features [id, empname, age, gender, compname, compaddr, jobtitle, hiredate, yearsofexp, salary]. Now one way of presenting a problem using this dataset is \"Given these features, determine the salary of this person with these features\", another can be \"Given these features, determine which gender is more likely to have these features\", etc. Depending on the problem, the data would be cleaned and the resultant dataset would be used for machine learning development. Lets say this model had 1500 records when loaded from the data resource and after preprocessing only 600 records are left, than considering the salary regression problem, one can say this data may not be sufficient to get a good result (assuming each feature covers up a diverse set of values) as only 40% of the original data is capable for the model development process. In cases with say less diverse features, this may also be enough to get a good result. So in the end, it depends on the type of problem and nature of the data being used. \n",
    "\n",
    "### Rule of Thumb:\n",
    "The general rule of thumb for data quantity is that the number of records in the dataset must be atleast 10x more than the number of features in a machine learning task but as mentioned previously, all this depends on the different factors discussed today.\n",
    "\n",
    "That's it for today! We will look into the next sub-step of Structural Investigation i.e. kinds of statistical variables in datasets next time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
