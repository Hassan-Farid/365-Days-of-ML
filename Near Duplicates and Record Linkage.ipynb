{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fd1304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary modules for this task\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39571efb",
   "metadata": {},
   "source": [
    "# Near Duplicates:\n",
    "Near Duplicates or Fuzzy Duplicates are a commonly arising problem in the process of deduplication. It refers to the set of those partial or full duplicates, that are slightly disimilar to their corressponding records but carry the same context i.e. these duplicates represent the same record (since they are duplicates) but have slight differences in their naming e.g. due to typos. misspellings, grammatical errors, synonyms, etc.\n",
    "\n",
    "# Causes for Near Duplicates:\n",
    "Near Duplicates are a frequently occurring problem in Machine Learning datasets as it happens to be the case, there are many reasons for near duplicates:\n",
    "\n",
    "## Data Entry Errors:\n",
    "The most common of the bunch in cases of machine learning tasks are data entry errors i.e. the value for the data entered has some typos or misspellings induced in it by the entry person or machine. Example of this case might be entering the record \"Jane\" instead of \"Jone\" for a student table, etc.\n",
    "\n",
    "## Multiple Entries:\n",
    "Another case of this can be a dataset being filled up by multiple people or groups at the same time. This results in each person or group saving the entries as per their set of information and may result in near duplicate entries. Example of this case might be two entry operators entering the records \"John Smith\" and \"Jon Smith\" for a student table, etc.\n",
    "\n",
    "## Text Translation:\n",
    "Near duplicates may also be the result of translation of one language to another. Example of this case is are the words \"Good Morning\" if converted to Japense translate to \"Ohayō\", which are different in syntax but represent the same context.\n",
    "\n",
    "## Plagirism:\n",
    "Another root cause of near duplicates is plagirism when generating content or data. This is frequent in content writing, critic reviews, news articles, school assignments, etc. For example, one critic may copy some other critic's lines and mix it with his own to create his review on a particular book, etc.\n",
    "\n",
    "# Removal of Near Duplicates:\n",
    "The process of removing near duplicates and identifying the records that represent the same real world entity is referred to as \"Entity Recognition\" or simply \"Record Linkage\". Since near duplicates are deemed as a very important issue in terms of machine learning, due to their effect on the efficiency of the models, many different techniques have been proposed to remove them from a given dataset. Each technique is based on the set of requirements for the given problem and can supposedly be distributed into two phases i.e. prepartion and similarity measurement.\n",
    "\n",
    "## Data Preparation Phase:\n",
    "In this phase, different types of techniques are used to prepare the data so as to perform similarity metrics on the records. The goal of data preparation phase is to use the data format required in accordance with the given problem and improvise the process of record linkage. Data preparation phase itself can be divided into two sub-phases i.e. preprocessing and extraction.\n",
    "\n",
    "### Data Preprocessing Phase:\n",
    "In this phase, the data is preprocessed so as to obtain an efficient format of data to generate relevant features from given variables to get an effective result for near duplicates. There are many data preprocessing techniques that can be used for this case, some of these are as follows:\n",
    "\n",
    "* Standardization\n",
    "* Sorting\n",
    "* Blocking\n",
    "* Sampling\n",
    "* Summarization\n",
    "\n",
    "### Feature Extraction Phase:\n",
    "In this phase, the data is provided to a certain function that results in helping in the identification of near duplicates. The resultant value for the function can be a fingerprint, set of tokens or a numerical vector, etc. There are many feature extraction techniques that can be used for this case, some of these are as follows:\n",
    "\n",
    "* N-gram Fingerprinting\n",
    "* Shingling\n",
    "* MinHash\n",
    "* Local Sensitivity Hashing (LSH)\n",
    "* Tf-Idf Vectorizer\n",
    "* Siamese Networks\n",
    "\n",
    "## Similarity Measurement Phase:\n",
    "Once the required features have been extracted from the given data, these features are provided to similarity metrics which allow to determine whether the records are near duplicates or not. Similarity Measures can be categorized as both character based as well as context based, but we are going to consider only character ones in this notebook. These include:\n",
    "\n",
    "* Levenshtein Distance\n",
    "* Jaccard Similarity\n",
    "* Sorenson-Dice Coefficient\n",
    "* Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cd312",
   "metadata": {},
   "source": [
    "# Data Processing Phase:\n",
    "We will now look at the different techniques mentioned in this phase, with the help of examples:\n",
    "\n",
    "## Data Preprocessing:\n",
    "The data preprocessing phase comprises of several techniques, which can either be used singly or in a combination with one another depending on the type of data we need for the feature extraction step. The most common preprocessing techniques that are mostly used in this case include standardization, filtering, encoding, etc. Lets now discuss the ones we mentioned step by step:\n",
    "\n",
    "### Data Standardization:\n",
    "Data Standardization is a preprocessing step that is used to standardize the records into a desired format. Standardization may include removing stop words from text, eliminating special characters, etc. The goal is to ensure that all the records in the given dataset are comparable to each other. This serves as a commonly used preprocessing step as it allows to generate records so that they can be easily compare for similarity measurement.\n",
    "\n",
    "#### Example:\n",
    "Lets consider the example of the two text records \"Karachi, Pakistan\" and \"karachi in Pakistan\". Removing the special characters and prepositions and lowercasing the result, we get the following standardized result for the two records \"karachipakistan\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e76f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized record for \"Karachi, Pakistan\": karachipakistan\n",
      "Standardized record for \"Karachi in Pakistan\": karachipakistan\n"
     ]
    }
   ],
   "source": [
    "#Loading records as a list\n",
    "records = [\"Karachi, Pakistan\", \"Karachi in Pakistan\"]\n",
    "\n",
    "#Function to generate the corresponding standardized versions of record \n",
    "def standardize_sp_prep(record):\n",
    "    '''\n",
    "        Method to standardize a record by removing its special characters and prepositions and giving result in lowercase format\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(record)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return ''.join(x[0].lower() for x in tagged if (not(x[1] == 'IN') and x[0].isalpha()))\n",
    "\n",
    "for record in records:\n",
    "    print('Standardized record for \"{}\": {}'.format(record, standardize_sp_prep(record)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b0157",
   "metadata": {},
   "source": [
    "### Character Sorting:\n",
    "Character Sorting is another preprocessing step that is helpful in the process of record linkage as it allows to align all the characters for a sentence in a particular order, thus, helping in cases where the records are in a reorder format. There are many different sorting algorithms out there which can be utilized to achieve this task. \n",
    "\n",
    "#### Example:\n",
    "Lets consider the example of two records \"He ate an apple yesterday\" and \"Yesterday, he ate an apple\". Applying standardization and then sorting the result for characters will give us the result \"aaaadeeeeehlnpprsttyy\" for both records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a25e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaaadeeeeehlnpprsttyy', 'aaaadeeeeehlnpprsttyy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the records\n",
    "records = [\"He ate an apple yesterday\", \"Yesterday, he ate an apple\"]\n",
    "\n",
    "#Function to generate character sorted versions of standardized records\n",
    "def sort_standard_records(records, standard_func):\n",
    "    records = [''.join(x for x in sorted(standard_func(record))) for record in records]\n",
    "    return records\n",
    "\n",
    "#Generate sorted versions for the given inputs\n",
    "sort_standard_records(records, standardize_sp_prep) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34945d3",
   "metadata": {},
   "source": [
    "### Blocking:\n",
    "Blocking is a preprocessing technique that allows to distinguish the different records of the dataset based on some criteria so as to reduce the number of comparisons to be made between the records of the same kind. This criteria depends on the type of application and can differ for different usecases. Once the blocking has divided the records into subsets, each subset can undergo the defined techniques to find out if they are near duplicates of each other.\n",
    "\n",
    "#### Example:\n",
    "Lets consider the example of determining the subset of records based on the department of the students i.e. given sample records of students of a university from different departments, we have to use blocking to generate subsets for each department, so that we can check for near duplicates for each sub-block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "905b033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsets for Mech: [{'name': 'John Smith', 'dept': 'Mech'}, {'name': 'John, Smith', 'dept': 'Mech'}]\n",
      "Subsets for Chem: [{'name': 'Jane Doe', 'dept': 'Chem'}]\n"
     ]
    }
   ],
   "source": [
    "#Loading the records\n",
    "records = [{'name':'John Smith', 'dept':'Mech'}, {'name':'Jane Doe', 'dept':'Chem'}, {'name':'John, Smith', 'dept':'Mech'}]\n",
    "\n",
    "#Method to define the blocking criteria \n",
    "def blocking_criteria(record):\n",
    "    return record['dept']\n",
    "\n",
    "#Method to generate blocks based on blocking criteria\n",
    "def generate_blocks(records, blocking_criteria):\n",
    "    subsets = defaultdict(list)\n",
    "    for record in records:\n",
    "        criteria = blocking_criteria(record)\n",
    "        subsets[criteria].append(record)\n",
    "    return subsets\n",
    "\n",
    "#Generate blocks for the given records\n",
    "subsets = generate_blocks(records, blocking_criteria)\n",
    "\n",
    "#Checking the subsets based on the defined criteria\n",
    "for criteria, record in subsets.items():\n",
    "    print(\"Subsets for {}: {}\".format(criteria, record))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fb9e8",
   "metadata": {},
   "source": [
    "### Sampling:\n",
    "Sampling is the process of randomly selecting a set of records from the given records and determine near duplicates for the selected sample. The goal of the sampling preprocessing step is the same as blocking i.e. to reduce the number of comparisons to make with other records for determination of near duplicates. It is to be noted that sampling in this case takes place without replacement so that no two same records can be repeated. \n",
    "\n",
    "#### Example:\n",
    "Lets consider an example for sampling where we will sample two records at a time (without replacement) from the provided records, which can later be utilized for detection of near duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550e16f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I love Karachi', 'I live in Karachi, Pakistan'],\n",
       " ['I lived in Karachi in Pakistan', 'I live in Pakistan']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the records\n",
    "records = [\"I live in Karachi, Pakistan\", \"I lived in Karachi in Pakistan\", \"I love Karachi\", \"I live in Pakistan\"]\n",
    "\n",
    "#Method to sample records without replacement\n",
    "def sample_without_replacement(records, sample_size):\n",
    "    random.shuffle(records)\n",
    "    sampled_records = []\n",
    "    for i in range(0, len(records), sample_size):\n",
    "        sampled_records.append(records[i:i+sample_size])\n",
    "    return sampled_records\n",
    "\n",
    "#Generting sample subsets for records of size 2\n",
    "sample_without_replacement(records, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b93c2bb",
   "metadata": {},
   "source": [
    "### Text Summarization:\n",
    "Text Summarization is the process of summarizing large piles of text so as to ease the process of feature extraction on them. In this case, near duplicates gets difficult to compute with the increase in size of text. Thus, in case the context of the passage is to be put into consideration, one can apply the process of text summarization to generate a smaller version of the text. Since the context is the same for the summarized text, thus, near duplicates can still be detected from the summarized ones.\n",
    "\n",
    "Since manual summarization is out of option, automatic summarization is opted for this step. Automatic summarization can take place in two ways i.e.\n",
    "\n",
    "* __Extractive Summarization__ - This method extracts only the important sentences and phrases from provided text and use them to generate the summary for given input\n",
    "\n",
    "* __Abstractive Summarization__ - This method compresses the provided input text by organizing the text to capture only the salient features of the source text and may embed new sentences as well.\n",
    "\n",
    "#### Example:\n",
    "Lets take into consideration a piece of text about \"Artificial Intelligence\" and summarize it. We can use text of similar kinds in a given dataset to summarize them and then find near duplicates between them. Here we are going to make use of extractive summarization to get the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37a31173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The field of AI research was founded on the belief that a machine can be made to think like a human if only we understand how the human mind works. AI has the potential to revolutionize various industries, from healthcare to transportation, and is already being used in a wide range of applications such as image recognition, natural language processing, and self-driving cars.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the records\n",
    "record = \"Artificial intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and learn. It has become one of the most rapidly growing and impactful technologies of the 21st century. AI systems can perform tasks such as recognizing speech, making decisions, and translating languages with a high degree of accuracy. The field of AI research was founded on the belief that a machine can be made to think like a human if only we understand how the human mind works. AI has the potential to revolutionize various industries, from healthcare to transportation, and is already being used in a wide range of applications such as image recognition, natural language processing, and self-driving cars. However, there are also concerns about the potential negative impacts of AI, such as job displacement and ethical issues. As AI continues to advance, it is important for society to consider both the benefits and the potential risks of this powerful technology.\"\n",
    "\n",
    "#Method to perform text summarization\n",
    "def get_record_summary(record):\n",
    "    #Generating a parsed document from the given record\n",
    "    parser = PlaintextParser.from_string(record, Tokenizer(\"english\"))\n",
    "    #Creating summary of the given text\n",
    "    summarizer = TextRankSummarizer()\n",
    "    #Generate summary using highest ranking sentences\n",
    "    summary = summarizer(parser.document, 2)\n",
    "    #Join the sentences to generate summary paragraph\n",
    "    return ' '.join(str(x) for x in summary)    \n",
    "        \n",
    "#Get summary for record\n",
    "get_record_summary(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb5c32",
   "metadata": {},
   "source": [
    "## Feature Extraction:\n",
    "The feature extraction phase makes use of the preprocessed data to extract important meanings from it in the form of features. These obtained features can then be utilized by similarity metrics to get a grip of the quantity of near duplicates that are present in the provided dataset. \n",
    "\n",
    "### N-Gram Fingerprinting:\n",
    "N-gram fingerprinting is a technique that generates fingerprints for records or documents by generating a set of non-overlapping N-grams for the words/characters in the standardized data provided to it. In case of large documents, it first uses a hash function so as to compress the size of the document to a fixed hash value, after which it generates the N-grams. It then compares the N-grams for each record to check for near duplicates. A variant of this is the sorted N-gram fingerprinting in which each N-gram is first sorted before comparison.\n",
    "\n",
    "#### Example:\n",
    "Consider the example of sorted N-gram fingerprinting for the records \"Yesterday, Ali went to the historic meuseum at Karachi, Pakistan\" and \"Ali, yesterday, went to the historic meuseum at Karachi in Pakistan\", giving us the same result. (Since the text isn't large enough, we won't use hashing in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3930fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['achalidayeumeushisicmipakarkistanterthetorttowenyes',\n",
       " 'achalidayeumeushisicmipakarkistanterthetorttowenyes']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading records for n-gram fingerprinting\n",
    "records = [\"Yesterday, Ali went to the historic meuseum at Karachi, Pakistan\",\n",
    "           \"Ali, yesterday, went to the historic meuseum at Karachi in Pakistan\"]\n",
    "\n",
    "#Method to generate sorted n_gram fingerprints for a record\n",
    "def generate_sorted_ngram_fingerprint(record, n):\n",
    "    #Standardize input\n",
    "    record = standardize_sp_prep(record)\n",
    "    #Generate ngrams\n",
    "    ngrams = [record[i:i+n] for i in range(0, len(record), n)]\n",
    "    #Sort generated ngrams to give a fingerprint\n",
    "    sorted_ngram = ''.join(sorted(ngrams))\n",
    "    return sorted_ngram\n",
    "\n",
    "#Generate fingerprints for both records (using n=3)\n",
    "fingerprints = [generate_sorted_ngram_fingerprint(record,3) for record in records]\n",
    "fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79debb29",
   "metadata": {},
   "source": [
    "### Shingling:\n",
    "Shingling is the process of generating shingles i.e. a subset of overlapping words from the tokenized standard text data provided to it. The number of words to allow in a shingle is set up using the value of parameter k. Once the shingles have been generated, they are hashed, with the resultant hashes used for similarity comparisons. The hashing step may not take place in cases of small records. One thing to remember is that in shingling, the value of k determines the accurracy of the shingles. If the value of k is low, the number of shingles produced are greater but the similarity is usually less. Similarly, if the value of k is high, the number of shingles produced is less but the similarity is usually greater.\n",
    "\n",
    "#### Example:\n",
    "Consider the example of shingling for the records \"John Smith studies with his buddies.\" and \"Jon Smith, studies with his buddyes.\", which give us similar results (using similarity measures we conclude whether the results produced are similar or not). We are using k = 2 for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42428675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['JohnSmith', 'Smithstudies', 'studieswith', 'withhis', 'hisbuddies'],\n",
       " ['JonSmith', 'Smithstudies', 'studieswith', 'withhis', 'hisbuddyes']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the records for shingling\n",
    "records = [\"John Smith studies with his buddies.\", \"Jon Smith, studies with his buddyes.\"]\n",
    "\n",
    "#Method to generate k-shingles from given record\n",
    "def generate_shingles(record, k):\n",
    "    #Standarizing the record\n",
    "    std_record = ''.join(x for x in record if (x.isalpha() or x==' '))\n",
    "    #Tokenizing the record\n",
    "    tokens = std_record.split()\n",
    "    #Generating shingles from tokens for k = 2\n",
    "    shingles = [''.join(tokens[i:i+k]) for i in range(len(tokens)-k+1)]\n",
    "    return shingles\n",
    "\n",
    "#Generating shingles for the provided records\n",
    "shingles = [generate_shingles(record, 2) for record in records]\n",
    "shingles #See that 3/5 are matching while 2/5 don't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9644eeda",
   "metadata": {},
   "source": [
    "### MinHash:\n",
    "MinHash is another common technique that is used to determine near duplicates from given dataset. The MinHash method relies on a number of steps, which are underlined as under:\n",
    "\n",
    "* Generate shingles for the given records for an optimal value of k\n",
    "* Assign a fixed number of integer buckets, such that some hash function h(x) can map each element in the produced shingles to exactly 1 bucket\n",
    "* Use the hash function to assign all items of the shingles to a bucket\n",
    "* For two different records, extract the minimum hash value obtained.\n",
    "* Check if the minimum hash values are same, if yes, then the two records are considered near duplicates of each other (as per Jaccard's index)\n",
    "\n",
    "#### Example:\n",
    "Consider the example of MinHash for the records \"John Smith studies with his buddies.\" and \"Jon Smith, studies with his buddyes.\", using a hash function to map each shingle of these records to an integer value, we get the following result from using MinHash: (Here we have used just a single hash function to showcase the process but in practice k different hash functions are used and the ratio x/k is used to determine the efficiency of the MinHash where x are the number of hash functions where the property of minHash satisfies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e54ef175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the records\n",
    "records = [\"John Smith studies with his buddies.\", \"Jon Smith, studies with his buddyes.\"]\n",
    "\n",
    "#Method for hashing function to use\n",
    "def hash_function(x, p):\n",
    "    hash_value = 0\n",
    "    for i, c in enumerate(x):\n",
    "        hash_value += ord(c) * (31 ** i)\n",
    "    return hash_value % p\n",
    "\n",
    "#Method for applying MinHash to the given records\n",
    "def apply_min_hash(record, k, p):\n",
    "    shingles = generate_shingles(record, k)\n",
    "    hashes = [hash_function(shingle, p) for shingle in shingles]\n",
    "    min_hash = min(hashes)\n",
    "    return min_hash\n",
    "\n",
    "#Generating minhash results for given records\n",
    "minhash_records = [apply_min_hash(record, 2, 11) for record in records]\n",
    "minhash_records #Gives same value for both records indicating they are near duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709400a",
   "metadata": {},
   "source": [
    "### Local Sensitive Hashing (LSH):\n",
    "Local Sensitive Hashing (LSH) is a technique to determine near duplicates in a dataset. LSH can be considered as the general implementation for MinHash and is implemented as follows:\n",
    "\n",
    "* Generate k-shingles for the given piece of text\n",
    "* Once the shingles have been generated, use MinHashing to generate MinHash value for each record\n",
    "* The MinHashing procedure is carried several times using multiple hash functions say k, giving us a k-sized signature.\n",
    "* The process of banding takes place on the obtained signatures for each record and dividing them into a set of sub-signatures, each of which is assigned to a band.\n",
    "* For each sub-signature, we use a hash function to generate the hash value and then put it accordingly into the corresponding band group.\n",
    "* The sub-signatures that turn out to be in the same group are considered similar in nature (i.e. represents pieces of text which are near duplicates in the given documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c7120",
   "metadata": {},
   "source": [
    "### Tf-Idf Vectorization:\n",
    "Tf-Idf (Term-Frequency Inverse-Document-Frequency) Vectorization is a technique that is used to map text into numerical vectors and can be utilized to detect near duplicate records. The vectorization process works by representing each record/document as a vector of Tf-Idf values which are computed by taking the product of the term frequency (number of occurrences of a term) and its inverse document frequency (measure of rarity of the term in the records/documents). Thus, this allows us to compare the similarity between the Tf-Idf vectors obtained for given records, which can tell us whether the records are near duplicates or not.\n",
    "\n",
    "#### Example:\n",
    "Consider the following two records: \"The cat sat on the mat\" and \"The feline was sitting on the carpet\", we will use the machine learning library scikit-learn to perform the process of tf-idf vectorization. The results obtained can then be used by similarity metrics to show whether the records are near duplicates or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bbe69b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'carpet': 0.0,\n",
       "  'cat': 0.42519636159088015,\n",
       "  'feline': 0.0,\n",
       "  'mat': 0.42519636159088015,\n",
       "  'on': 0.30253071324069974,\n",
       "  'sat': 0.42519636159088015,\n",
       "  'sitting': 0.0,\n",
       "  'the': 0.6050614264813995,\n",
       "  'was': 0.0},\n",
       " {'carpet': 0.39129369358468363,\n",
       "  'cat': 0.0,\n",
       "  'feline': 0.39129369358468363,\n",
       "  'mat': 0.0,\n",
       "  'on': 0.2784086857278066,\n",
       "  'sat': 0.0,\n",
       "  'sitting': 0.39129369358468363,\n",
       "  'the': 0.5568173714556132,\n",
       "  'was': 0.39129369358468363}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the records\n",
    "records = [\"The cat sat on the mat\", \"The feline was sitting on the carpet\"]\n",
    "\n",
    "#Method to compute tf-idf vectors for given records\n",
    "def compute_tfidf_vectorizers(records):\n",
    "    # Create an instance of TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit the vectorizer on the records\n",
    "    tfidf_vectors = vectorizer.fit_transform(records)\n",
    "\n",
    "    # Get the feature names\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    # Store tf-idf values in a list of dictionaries\n",
    "    result = []\n",
    "    for i, record in enumerate(records):\n",
    "        temp = {}\n",
    "        for j, feature in enumerate(feature_names):\n",
    "            temp[feature] = tfidf_vectors[i,j]\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "#Generating resultant tf-idf vectors for records\n",
    "compute_tfidf_vectorizers(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec26d68",
   "metadata": {},
   "source": [
    "### Siamese Networks:\n",
    "Siamese Networks are a type of neural network architecture that is used to learn similarity function between pairs of input data. The main idea behind this type of network is to use a shared set of weights to process the pair provided to it and make use of some sort of similarity measure to check if the two match or not. This property of Siamese networks allows it to be a good representative for identifying near duplicates in text. \n",
    "\n",
    "A basic approach to acheive this is by using a LSTM layer and pass the given inputs to this shared layer. The resultant output from the layer would be its last hidden state, which would give us a representation for our inputs that can be utilized by similarity metrics to generate whether they are near duplicates or not. Other approaches can also take place depending on the type of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229187b6",
   "metadata": {},
   "source": [
    "# Similarity Measurement Phase:\n",
    "We will now look at the similarity measurement phase, in which we try to find out the degree of similarity between the obtained features from given records\n",
    "\n",
    "### Levenshtein Distance:\n",
    "Levenshtein Distance is a character based similarity metric that measures the minimum number of string operations i.e. insertions, deletions and substitutions that are required to make the given pair of record same. It is sometimes generally referred to as \"Edit Distance\", though it is just a variant of it. A variant of Levenshtein Distance (called  Damerau–Levenshtein distance ) makes use of transpositions as well to compute the difference.\n",
    "\n",
    "#### Example:\n",
    "Consider the two records \"John Smith studies with his buddies.\" and \"Jon Smith, studies with his buddyes.\" for which we have to compute the levenshtein distance, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57100de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the records\n",
    "records = [\"John Smith studies with his buddies.\", \"Jon Smith, studies with his buddyes.\"]\n",
    "\n",
    "#Method to compute Levenshtein Distance between pair of records\n",
    "def compute_levenshtein_distance(record1, record2):\n",
    "    distance = 0\n",
    "    count = 0\n",
    "    iters = min(len(record1), len(record2))\n",
    "    while (count < iters):\n",
    "        if record1[count] != record2[count]:\n",
    "            distance += 1\n",
    "        count += 1\n",
    "    distance += len(record1[count:]) + len(record2[count:])\n",
    "    return distance\n",
    "\n",
    "#Computing levenshtein distance for the texts\n",
    "compute_levenshtein_distance(records[0], records[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c0fec",
   "metadata": {},
   "source": [
    "### Jaccard Similarity:\n",
    "Jaccard Similarity Coefficient is a metric used for measuring similarity of the number of occurrences of particular words/characters in two different records. The metric is defined as the number of unique words/characters common in the two sets divided by the total number of unique words/characters in the two sets. Thus, if we represent the number of words/characters as a set for each record, then jaccard similarity coefficient for the two records can be given by:\n",
    "\n",
    "<p><center>$J(A,B) = \\frac{A \\cap B}{A \\cup B}$</center></p>\n",
    "\n",
    "#### Example:\n",
    "Consider the following two records whose similarity is to be computed via the help of Jaccard similarity i.e. we have \"The old man went to the shop\" and \"The old lady went to the shop\", then based on the defined formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eaa42ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the records to handle\n",
    "records = [\"The old man went to the shop\", \"The old lady went to the shop\"]\n",
    "\n",
    "#Method to compute Jaccard similarity\n",
    "def compute_jaccard_similarity(record1, record2):\n",
    "    record1_set = set(record1.split(' '))\n",
    "    record2_set = set(record2.split(' '))\n",
    "    score = len(record1_set.intersection(record2_set)) / len(record1_set.union(record2_set))\n",
    "    return score\n",
    "\n",
    "#Computing jaccard score for given records\n",
    "compute_jaccard_similarity(records[0], records[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df4bef",
   "metadata": {},
   "source": [
    "### Sorenson-Dice Coefficient:\n",
    "Sorenson-Dice Coefficient is yet another metric that can provide insight about the similarity of two records based on the characters. The coefficient is given by computing the common characters/words in the given records divided by the total number of elements in both records. Thus, if we represent the number of characters/words as a set of records, then sorenson-dice coefficient is given by:\n",
    "\n",
    "<p><center>$DSC = \\frac{2|A \\cap B|}{|A|+|B|}$</center></p>\n",
    "\n",
    "#### Example:\n",
    "Consider the following two records whose similarity is to be computed via the help of Sorenson-Dice similarity i.e. we have \"The old man went to the shop\" and \"The old lady went to the shop\", then based on the defined formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ead3189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the records to handle\n",
    "records = [\"The old man went to the shop\", \"The old lady went to the shop\"]\n",
    "\n",
    "#Method to compute Sorenson-Dice similarity\n",
    "def compute_dice_similarity(record1, record2):\n",
    "    record1_set = set(record1.split(' '))\n",
    "    record2_set = set(record2.split(' '))\n",
    "    score = (2 * len(record1_set.intersection(record2_set))) / (len(record1_set) + len(record2_set)) \n",
    "    return score\n",
    "\n",
    "#Computing sorenson-dice score for given records\n",
    "compute_dice_similarity(records[0], records[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12656dfa",
   "metadata": {},
   "source": [
    "### Cosine Similarity:\n",
    "Cosine Similarity is a metric for measuring the similarity of text in two records/documents and does so by making use of numeric vector representations of the inputted records. It makes use of non-zero vectors of the records and uses them to determine the value of cosine angle between them (making use of their magnitudes and dot product). This obtained value of cosine of given angle lies in the range [-1,1] with 1 indicating an angle of 0 degree (fully identical) and -1 indicating an angle of 180 degrees (completely opposite). The formula for computing the angle of cosine for given two vectors, we have:\n",
    "\n",
    "<h3><center>$cos({\\theta_{(A,B)}}) = \\frac{\\bar{A} . \\bar{B}}{|A||B|}$</center></h3>\n",
    "\n",
    "Cosine similarity is ususally used for measure similarity after converting the records under consideration into their numerical vector representations via some sort of word embedding feature extraction techniques e.g. GloVe, Word2Vec, etc. In other cases, it can also be utilized by other vectorization techniques such as Tf-Idf vectorization to get satisfactory results.\n",
    "\n",
    "#### Example:\n",
    "Consider the following two records: \"The cat sat on the mat\" and \"The feline was sitting on the carpet\", we will use the machine learning library scikit-learn to perform the process of tf-idf vectorization. The obtained results would then be fed to the cosine similarity method to measure the degree of similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38ab9b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5023287782256718"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the records\n",
    "records = [\"The cat sat on the mat\", \"The feline was sitting on the carpet\"]\n",
    "\n",
    "#Method to compute cosine similarity between two vector representations\n",
    "def compute_cosine_similarity(vec1, vec2):\n",
    "    return (vec1 @ vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "#Standardize the records\n",
    "for i in range(len(records)):\n",
    "    records[i] = ''.join(x for x in records[i] if x not in stopwords.words('english'))\n",
    "\n",
    "#Generating resultant tf-idf vectors for records\n",
    "tfidf_vectors = compute_tfidf_vectorizers(records)\n",
    "\n",
    "#Extracting vector representations for the two records\n",
    "vec1, vec2 = [np.array(list(vector.values())) for vector in tfidf_vectors]\n",
    "\n",
    "#Computing the cosine similarity between the vectors\n",
    "compute_cosine_similarity(vec1, vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb6d28",
   "metadata": {},
   "source": [
    "Here I have covered only a glimpse of the techniques and measures that are used for the process of text/document similarity. There are metrics that identify whether the two inputs have same semantic meaning, there are techniques to improvise the efficiceny of determining these measures and there are tons of techniques and researches going out there that are targetting the problem of removing near duplicates from datasets. That being said, this notebook just touched over the mere basics of what possibly can be used to reduced this problem. This is where we end the topic of Duplicate Handling for datasets, we will start looking at the topic of missing values starting tomorrow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
