{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b8b7b2",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "Text Preprocessing is one of the essential stages in training a Natural Language Processing (NLP) based machine learning model. Text Preprocessing allows to process the textual data and retrieve a representation for textual data that is well-suited for the machine learning model being implemented. There are various kinds of techniques are out there for processing textual data. I will cover many of these techniques here (usually it is achieved by making a use of mix of the following techniques in a certain combination). I will make use of Python to code out how we can perform these transformations.\n",
    "\n",
    "## About the Dataset:\n",
    "The dataset used in this notebook is taken from Kaggle's [Sentiment140 dataset with 1.6 million tweets](https://www.kaggle.com/datasets/kazanova/sentiment140) and is targetted to perform sentiment analysis on the given tweets. In the notebook, I am going to use example demonstrations for each of the preprocessing technique mentioned, keeping in mind the goal of the dataset and thus will make examples sentiment analysis centric i.e. how one can use the given technique to achieve a result that might help in performing sentiment analysis. So lets begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a392276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "from autocorrect import Speller\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fa15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "tweets = pd.read_csv('./sample datasets/sentiment140_tweets.csv', names=columns, header=None, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c78b049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the data\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7155d",
   "metadata": {},
   "source": [
    "## Preprocessing Techniques:\n",
    "Following are the different preprocessing techniques that we will be taking a look at, with examples targetted towards how it can be helpful in case of sentiment analysis:\n",
    "\n",
    "1. Word Tokenization\n",
    "2. Case Adjustment\n",
    "\n",
    "### 1. Word Tokenization:\n",
    "The most common as well as necessary preprocessing technique in text processing is to generate tokens from given piece of text. Although these tokens can be generated on the required grain level (i.e. can be sentence tokenization or even character tokenization), but this is the mostly used case out of all. Word tokenization comes into play in NLP tasks where the analysis or prediction has to be carried out on the basis of words. The process makes use of the fact that words are seperated by single or more spaces and uses these whitespaces in the given text corpus to generate the set of words used in it. \n",
    "\n",
    "#### Example Demonstration:\n",
    "To perform sentiment analysis, you would need to tokenize each tweet on the word level to get a gist of which words are being used in it. Since we can categorize each word on the positive-negative scale, we can get a good idea of whether the text represents positivity or negativity. Lets take a look at a sample. (Take into consideration that we have put a check on if the obtained tokens contain '' or not as it doesn't count as a word and may be present in the tokens if we use single space as the delimiter for tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0feee38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@friskyupdater', 'my', 'ym', \"isn't\", 'working.']\n",
      "['gettin', 'my', 'tan', 'on']\n",
      "['bout', '2', 'Order', 'my', 'soy', 'vanilla', 'rooibos', 'tea', 'latte', '.Brb', 'Lovez']\n",
      "['@elleLOVESgreys', 'OMFG', '...', 'hell', 'yes!', 'It', 'was', 'so', 'MerDer', 'of', 'them!', 'I', 'loved', 'that', 'Der', 'did', 'it', 'cos', 'Mer', 'was', 'freakin', 'out', 'about', 'having', 'no', 'time']\n",
      "['Would', 'really', 'rather', 'stay', 'home', 'and', 'get', 'on', 'with', 'my', 'work']\n"
     ]
    }
   ],
   "source": [
    "#Defining method to tokenize text into a set of words on presence of any spaces\n",
    "def tokenize_words(text):\n",
    "    return [x for x in text.split(' ') if x != '']\n",
    "\n",
    "#Selecting 5 random tweets for tokenization\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "for tweet in random_tweets:\n",
    "    print(tokenize_words(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c9a70",
   "metadata": {},
   "source": [
    "### 2. Case Adjustment\n",
    "The most basic kind of preprocessing technique is changing the case for each of the words in the given piece of text. This technique makes use of a certain standard case format which is to be applied to each word. This is mostly useful in cases, where the same word has been defined in the input text with different casings and we are interested in finding the number of each word's occurrences. \n",
    "\n",
    "#### Example Demonstration:\n",
    "In the case of sentiment analysis, one thing that is very common to compute is the word frequency. Word frequency tells us how many times a certain word is repeated in the given text. But given the fact that a text may contain the same word but in different cases, the frequency computation may result in counting the same word in different formats as seperate entities. To avoid this, we need to convert each word into a standard case format (usually lowercase is used). Lets take a look at a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b555437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@cgney', 'y', \"didn't\", 'u', 'come', 'across', 'da', 'street', '2day']\n",
      "['@bethluvsbubbles', 'haha', 'lol', 'at', 'least', 'u', 'dont', 'have', 'to', 'write', 'a', 'history', 'essay', 'that', 'i', 'havent', 'even', 'started', 'and', 'ahs', 'to', 'be', 'in', 'tomoz', 'lol']\n",
      "['is', 'sat', 'in', 'front', 'of', 'his', 'computer', 'on', 'the', 'last', 'day', 'of', 'the', 'holidays']\n",
      "['@simonmagus', 'aww.thats', 'not', 'good!', 'especially', 'when', 'the', 'best', 'tennis', 'court', 'in', 'the', 'world', 'belongs', 'to', 'ur', 'county!', 'jokin,no', 'offence.']\n",
      "['@themarsfactory', 'hahahah!']\n"
     ]
    }
   ],
   "source": [
    "#Defining method to convert all words in a tokenized sentence to lowercase (or any other standard case as per choice)\n",
    "def adjust_lowercase(tokens):\n",
    "    return [x.lower() for x in tokens]\n",
    "\n",
    "#Selecting 5 random tweets for case adjustment\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "for tweet in random_tweets:\n",
    "    tokenized = tokenize_words(tweet)\n",
    "    print(adjust_lowercase(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7711c",
   "metadata": {},
   "source": [
    "### 3. Stopword Removal:\n",
    "Wikipedia defines the definition of stopwords as follows:\n",
    "\n",
    "> Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant\n",
    "\n",
    "So removing stopwords from the given piece of text gives a better insight about the text as a whole as all the unnecessary words are cleared out. There are different implementations of stopword removal present (even custom ones are present and can be used) each with their set of words defined as stopwords. Applying stopword removal should take into account the problem under consideration as well e.g. if your goal is to detect sentiment from a piece of text than obviously words like \"a\", \"is\", \"the\" don't carry that much meaning but if the goal is that of checking names of something it might not be a good idea as there might be items with the same name with a different variant of stopwords to discriminate them e.g. the movies \"Cloud Atlas\" by David Mitchell and \"The Cloud Atlas\" by Liam Callanan would be considered the same entity if the stopwords were removed, although both are quite different.\n",
    "\n",
    "#### Example Demonstration:\n",
    "For the case of sentiment analysis, the stopwords usually don't put up much sentiment into the given piece of text and thus aren't needed to find the emotional stability of the given piece of text. Therefore, we perform stopword removal in this case to reduce the noise of the text so that more accurate analysis can be performed. Lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b351b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ah', 'poo', 'stavros', 'love', 'dad!']\n",
      "['neither', 'movies', 'mom', 'wanted', 'see', 'birthday', 'playing', 'naperville', 'wtf?', 'woody', 'allen', 'flick', 'away', 'go.', 'directtv']\n",
      "[\"uno's\", 'family', 'chane`s', 'graduation!']\n",
      "['@kaaatiee', 'happy', 'birthdayy', 'hope', 'good', 'one.']\n",
      "[\".....i'm\", 'movin', 'storage', 'like', \"i'm\", 'boy...my', 'daddy', 'want', 'boy!!']\n"
     ]
    }
   ],
   "source": [
    "#Defining method to remove all stopwords from tokenized words (we use english since thats the language used for text)\n",
    "def remove_stopwords(tokens):\n",
    "    return [x for x in tokens if x not in stopwords.words('english')]\n",
    "\n",
    "#Selecting 5 random tweets for stopword removal\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "for tweet in random_tweets:\n",
    "    tokenized = tokenize_words(tweet)\n",
    "    lower_tokenized = adjust_lowercase(tokenized)\n",
    "    print(remove_stopwords(lower_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a157651a",
   "metadata": {},
   "source": [
    "### 4. Stemming:\n",
    "Stemming is defined by Wikipedia as follows:\n",
    "\n",
    "> In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n",
    "\n",
    "Stemming refers to the process of reducing the given word in such a manner that we get either the root word or something close to it. By something close, I mean that the result generated from stemming doesn't need to be the root word itself as per the defined linguistic e.g. in case of the word \"studying\" we get the stem word \"study\" which infact is the correct root word but if we use the same approach for the word \"studies\", it would give us the result \"studi\" as the result which is not the root word but closely resembles it (should have been study). \n",
    "\n",
    "#### Example Demonstration:\n",
    "For the case of sentiment analysis, we know that root words are the ones that carry the sentiment of the word and that affixes are just there to generate a variant of the word as per the grammar rules fitting the sentence. Thus, removing affixes from the word to get root stems improves the efficiency of the results. Lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9779409d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['school', 'take', 'test', 'soon.']\n",
      "['mom', 'one', 'best!!!']\n",
      "['today', 'one', 'bestest', 'day', 'entir', 'year', '&lt;3', 'dolor', '!', 'datelin', '1', 'hout', 'dont', 'forget', 'watch', 'it!!!', '@taylorswift13']\n",
      "['@thewesleychan', 'oh', 'boo', 'boston', 'meet', 'you!', 'fun', 'there!']\n",
      "['@yelitout', 'hi', 'fig', '!', 'good', '4', 'u', 'dont', 'get', 'talk']\n"
     ]
    }
   ],
   "source": [
    "#Defining method to generate stem words for each token word\n",
    "def generate_stems(tokens):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return [stemmer.stem(x) for x in tokens]\n",
    "\n",
    "#Selecting 5 random tweets for stemming\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "for tweet in random_tweets:\n",
    "    tokenized = tokenize_words(tweet)\n",
    "    lower_tokenized = adjust_lowercase(tokenized)\n",
    "    stopword_tokenized = remove_stopwords(lower_tokenized)\n",
    "    print(generate_stems(stopword_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335bb6d",
   "metadata": {},
   "source": [
    "### 5. Lemmatization:\n",
    "Wikipedia defines Lemmatization as:\n",
    "\n",
    "> Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "Lemmatization is the process of extracting the lemma from the given word i.e. the base word that not only can be referred to as the root form for a pair of similar words but also represents contextual meaning from a linguistic point of view. The difference between stemming and lemmatization is that stemming gives a root word without affixes as a result but that may not be a real word but on the other hand lemmatization will always give a base word that carries the intended meaning of the word. A very basic example of this is the word \"betterment\", which for a stemming algorithm would return the result \"better\" but for lemmatization would return \"good\", taking into account not only the characters rather the meaning.\n",
    "\n",
    "#### Example Demonstration:\n",
    "For the case of sentiment analysis, it is better to obtain the lemmatized form for each word rather than the stem word. This is because this results in increasing the frequency count of the same intended words, increasing the chance of detecting whether the text carries positive or negative meaning as a whole. As explained in the above passage, stemming would quantify \"good\" and \"better\" as different words whereas lemmatization would consider them the same, increasing the overall positivity of the text. Lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f957dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waiting', 'man', 'get', 'hour', 'go', 'look', 'house']\n",
      "['arr', 'never', 'ever', 'felt', 'like', 'quite', 'much', 'shit', 'before.', 'shit.', 'massive', 'shit.']\n",
      "['3rd', 'frequent', 'visitor', 'thatfleminggent.com', '#ipv6', 'address.', \"there's\", 'hope']\n",
      "['happy', '159', 'followers...then', 'saw', '@alainvanheerden', '23,775', '-', 'feeling', 'small', 'fish!']\n",
      "['coming', 'home.', 'super', 'loud', 'got', 'coooooool', 'shirt']\n"
     ]
    }
   ],
   "source": [
    "#Defining method to generate lemmas for each token word\n",
    "def generate_lemmas(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(x) for x in tokens]\n",
    "\n",
    "#Selecting 5 random tweets for lemmatization\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "for tweet in random_tweets:\n",
    "    tokenized = tokenize_words(tweet)\n",
    "    lower_tokenized = adjust_lowercase(tokenized)\n",
    "    stopword_tokenized = remove_stopwords(lower_tokenized)\n",
    "    print(generate_lemmas(stopword_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f60f7",
   "metadata": {},
   "source": [
    "### 6. Punctuation Removal:\n",
    "Punctuation are special characters such as \",\",\".\",\"!\",etc. that are included in text to emphasize the silent intonation. Punctuation characters like \"!\" indicate exclaimination which depending on the word may indicate anger, sorrow, surprise, etc. Similarity \".\" indicates where we pause on a certain piece of text and \"?\" indicates that the sentence itself is pointed out as a question to others. Punctuation removal in many cases gives an edge in training a model more accurately as it is just an addition of some special characters with no meaning to context. However, in some other cases, punctuation can be very helpful and must be kept in the text.\n",
    "\n",
    "#### Example Demonstration:\n",
    "In sentiment analysis, punctuation plays a good role in the fact that characters like \"!\", \".\" and \"?\" indicate the degree to which the positivity or negativity or any other emotion is present. For example, the word \"Come here.\" and \"Come here!!!\" are same non-punctual wise, but when appearing in text, the first one seems like a pretty normal tone whereas the second one seems as someone is screaming it out. Hence, in our case, we will likely keep punctuations that indicate the mentioned three characters and remove all the other ones present (Remember that we have to keep only the ones that are present at the end of the word as all others may be considered as garbage special characters). Lets take a look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b54a7143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brushing', 'teeth', 'tounge', 'mourning', 'new', 'adventure', 'pain']\n",
      "['tragic', 'movie!']\n",
      "['oh', 'good', 'lord.', 'today', 'over?']\n",
      "['bummed.', 'wasnt', 'able', 'watch', 'star', 'trek', 'chooi', 'georgey', 'porgey', 'today', 'noooooo!']\n",
      "['ommmgg', 'shoulder', 'killing', 'me!!!', 'wow', 'hurt', 'soo', 'much']\n"
     ]
    }
   ],
   "source": [
    "#Defining method to remove punctuation characters from a token (except ., ! and ?)\n",
    "def remove_word_punctuations(token):\n",
    "    exceptions = ['.','!','?']\n",
    "    temp = ''\n",
    "    for i in range(len(token)-1, 0, -1):\n",
    "        if token[i] in exceptions:\n",
    "            temp += token[i]\n",
    "        else:\n",
    "            temp = ''.join(x for x in token[:i+1] if x.isalnum()) + temp[::-1]\n",
    "            break\n",
    "    if temp:\n",
    "        return temp\n",
    "    else:\n",
    "        return token\n",
    "    \n",
    "#Defining method to remove punctuation characters from given tokens\n",
    "def remove_punctuations(tokens):\n",
    "    return [remove_word_punctuations(token) for token in tokens]\n",
    "    \n",
    "#Selecting 5 random tweets for punctuation removal\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "for tweet in random_tweets:\n",
    "    tokenized = tokenize_words(tweet)\n",
    "    lower_tokenized = adjust_lowercase(tokenized)\n",
    "    punctuated_tokenized = remove_punctuations(lower_tokenized)\n",
    "    stopword_tokenized = remove_stopwords(punctuated_tokenized)\n",
    "    print(generate_lemmas(stopword_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68160c3",
   "metadata": {},
   "source": [
    "### 7. Number Removal:\n",
    "Numbers are a big part of textual data as well as they are used to represent numeric quantities being targetted. These may include the size of some item, the number of frequency of some item or some sort of price related to an item. Number Removal depends on the task of preprocessing being done. If the task is associated with something that doesn't make any use of numbers, then it should by all means be removed but otherwise if numbers carry some usefulness in prediction analysis, then they should be kept. \n",
    "\n",
    "#### Example Demonstration:\n",
    "In sentiment analysis, it really depends on how are you going to handle the text. In cases where we have to take into consideration numerical attributes e.g. stock market pricing and such scenarios, we usually keep the numbers as they tell the positive and negative impacts on the market. Same goes for item prices, etc. But other than that, numbers are usually removed as they don't carry any sentiment value themselves. In our case, we are going to remove all numbers that occur between characters (as it represents words that don't make any sense - downside of this is that currencies which use words to represent them will also get removed e.g. 40$ or 40Rs and that is a risk we are going to take since there is a low probability of getting many tweets related to that topic from this random collection). We will keep just the numbers just for the sake of expressing the degree of how much influence is present e.g. a sample of the sort \"50 times happier\" shows the extent of positivity present in the text. Lets take a look:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50957f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insomnia', 'grace', 'u', 'presence', 'tonighthow', 'long', 'stay', 'time?', 'someone', 'talk']\n",
      "['im', 'diggin', 'new', 'job']\n",
      "['4jradio', 'oh', 'no!', 'well', 'thats', 'good']\n",
      "['happy', 'birthday', 'bobbysofamous', 'itsmrq', 'lt3', 'lot', 'great', 'day', 'guys!']\n",
      "['hate', 'exam', 'kancruut', 'ah']\n"
     ]
    }
   ],
   "source": [
    "#Defining methods to check if string is integer convertible or not\n",
    "def is_int(x):\n",
    "    try:\n",
    "        x = int(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def is_float(x):\n",
    "    try:\n",
    "        x = float(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "#Defining method to remove numbers within strings from a token\n",
    "def remove_word_numbers(token):\n",
    "    if is_int(token) or is_float(token):\n",
    "        return token\n",
    "    else:\n",
    "        return [x for x in token if x.isalpha()]\n",
    "    \n",
    "#Defining method to remove number characters from given tokens\n",
    "def remove_numbers(tokens):\n",
    "    return [remove_word_numbers(token) for token in tokens]\n",
    "    \n",
    "#Selecting 5 random tweets for number removal\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "for tweet in random_tweets:\n",
    "    tokenized = tokenize_words(tweet)\n",
    "    lower_tokenized = adjust_lowercase(tokenized)\n",
    "    punctuated_tokenized = remove_punctuations(lower_tokenized)\n",
    "    number_tokenized = remove_numbers(punctuated_tokenized)\n",
    "    stopword_tokenized = remove_stopwords(punctuated_tokenized)\n",
    "    print(generate_lemmas(stopword_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a455e6d",
   "metadata": {},
   "source": [
    "### 8. Pattern Extraction:\n",
    "Just like numbers and punctuations, there are other types of special patterns present in a text that can either add value or decrease the predictability factor. These patterns are not defined and need to be generated as a form of regular expressions for searching out. Examples of such patterns include finding URLs, emojis, emails, phone numbers, etc. Depending on the use case, these patterns may either help in improving the performance and can be kept, otherwise they are removed.\n",
    "\n",
    "#### Example Demonstration:\n",
    "In our case, since we have kept numbers and they may or may not be used for representing phone numbers, we are going to ignore them and keep them as it is. The things we are concerned with thought are two things for sentiment analysis. The first are the emojis as in this age of social media, there are very few posts where the use of emojis does not come into practice. The second ones are URLs (as they provide a link to some content that the tweet is about which itself means that we can get an even better idea of the sentiment of the user if we were to crawl out the content from the URL). Thus, this step instead of performing an inplace filtering is used to extract these patterns from the tokens and store them as new items in the list. Another thing that should be extracted though has been overlooked up till now is extracting usernames being used in tweets. Usernames start with the @ symbol, so we are going to assume any set of characters that comes after starting @ is a username. Lets take a look: (Don't wonder why no emojis or URLs show up since its very rare for them to occur for a random chance of 5 records, we could have avoided this wholely but since it is utilized in most sentiment tasks, I thought it would be worth mentioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3787ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lymph', 'node', 'hurt.']\n",
      "['awwws', 'miss', 'big', 'sister', 'emily!', 'daily', 'phone', 'call', 'make', 'cry']\n",
      "['finished', 'brother', 'sister', '2', 'eps', '11']\n",
      "['need', 'unlock', 'one', 'thats', 'problem', 'im', 'tmoblie', 'user']\n",
      "['awesome', 'graduation', 'party..', 'not.', 'sleeping', 'work', '5am', 'tomorrow!', 'im', 'bummed', 'miss', 'raymond.', 'super', 'super', 'lame...']\n"
     ]
    }
   ],
   "source": [
    "#Defining methods to find specific patterns from a token and returning them\n",
    "def get_patterns_from_tokens(tokens):\n",
    "    valid_url_regex = re.compile(\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\")\n",
    "    usernames = [x for x in tokens if x.startswith('@')]\n",
    "    valid_urls = [x for x in tokens if valid_url_regex.search(x)]\n",
    "    emojis = [emoji.emoji_list(token) for token in tokens]\n",
    "    return usernames, valid_urls, emojis\n",
    "\n",
    "def remove_usernames_from_tokens(tokens):\n",
    "    return [x for x in tokens if not x.startswith('@')]\n",
    "\n",
    "#Selecting 5 random tweets for url and emoji selection\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "for tweet in random_tweets:\n",
    "    tokenized = tokenize_words(tweet)\n",
    "    usernames, urls, emojis = get_patterns_from_tokens(tokenized)\n",
    "    removed_username_tokenized = remove_usernames_from_tokens(tokenized)\n",
    "    lower_tokenized = adjust_lowercase(removed_username_tokenized)\n",
    "    punctuated_tokenized = remove_punctuations(lower_tokenized)\n",
    "    number_tokenized = remove_numbers(punctuated_tokenized)\n",
    "    stopword_tokenized = remove_stopwords(punctuated_tokenized)\n",
    "    lemma_tokenized = generate_lemmas(stopword_tokenized)\n",
    "    print(lemma_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f58bd",
   "metadata": {},
   "source": [
    "### 9. Spell Correction:\n",
    "Many users have low knowledge of spellings especially in case of people foreign to that particular language. Despite that English is a very common medium among different countries, there are still a major popularity out there that does not have good know how of how to spell some words correctly. So is the case for other languages as well. Spelling correction is the process of making use of edit distance and other similarity measures to correctly spell a word if it is not right as per the dictionary. \n",
    "\n",
    "#### Example Demonstration:\n",
    "Spell Correction if done right is very helpful in case of sentiment analysis as it can automatically correct the words that either provide positive or negative influence and allow the model being trained on the data to improve its efficiency. This on the other hand can also result in declinment of results if the spell correction process somehow misinterprets a word for what it actually is. Lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f5f847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'wow', 'dont', 'know', 'prof', 'made', 'u', 'learn', 'portray...', 'know', 'portray', 'is?', 'knot', 'brain!']\n",
      "['hi', 'david..', 'im', 'philippines..', 'thanks', 'message', 'youtube..', 'hahalolim', 'big', 'fan..']\n",
      "['good', 'point', 'sir', 'good', 'point.', 'wanna', 'see', 'ufc', '100', 'dont', 'fuck', 'ufc.']\n",
      "['frank', 'intra', 'boss!', 'frankie', 'day', 'yesterday!']\n",
      "['love']\n"
     ]
    }
   ],
   "source": [
    "#Defining method to perform autocorrection for spellings\n",
    "def autocorrect_tokens(tokens):\n",
    "    speller = Speller(lang='en')\n",
    "    return [speller(x) for x in tokens]\n",
    "\n",
    "#Selecting 5 random tweets for autocorrecting\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "for tweet in random_tweets:\n",
    "    tokenized = tokenize_words(tweet)\n",
    "    usernames, urls, emojis = get_patterns_from_tokens(tokenized)\n",
    "    removed_username_tokenized = remove_usernames_from_tokens(tokenized)\n",
    "    lower_tokenized = adjust_lowercase(removed_username_tokenized)\n",
    "    punctuated_tokenized = remove_punctuations(lower_tokenized)\n",
    "    number_tokenized = remove_numbers(punctuated_tokenized)\n",
    "    stopword_tokenized = remove_stopwords(punctuated_tokenized)\n",
    "    lemma_tokenized = generate_lemmas(stopword_tokenized)\n",
    "    print(autocorrect_tokens(lemma_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0cf9f",
   "metadata": {},
   "source": [
    "### 10. Numerical Encoding:\n",
    "Numerical Encoding refers to generating a numerical representation for tokenized text so that the machine learning model can get a better grasp of which words or sentences help in the generation of the obtained results. There are different numerical encoding techniques available e.g. CountVectorizer, Tfidf Vectorizer, Word2Vec, GloVe, etc. but here we are going to use tf-idf as it is one of the most common and useful ones.\n",
    "\n",
    "Tf-Idf (Term-Frequency Inverse-Document-Frequency) Vectorization is a technique that is used to map text into numerical vectors and can be utilized to detect near duplicate records. The vectorization process works by representing each record/document as a vector of Tf-Idf values which are computed by taking the product of the term frequency (number of occurrences of a term) and its inverse document frequency (measure of rarity of the term in the records/documents). The advantage of this is that since it makes use of the whole document to generate a numerical representation, it is not bound to the length of the document and still captures the relevant features.\n",
    "\n",
    "#### Example Demonstration:\n",
    "Tf-Idf can be useful for sentiment analysis in the sense that it allows to capture the meaning of each word in context with the tweet in which it has been used. This allows the machine learning model to be trained to get a better idea of how much influence does each word in the document has in resulting a positive or negative target result. Lets take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eefd357f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.35355339,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.35355339,\n",
       "        0.        , 0.        , 0.        , 0.35355339, 0.        ,\n",
       "        0.35355339, 0.35355339, 0.        , 0.35355339, 0.        ,\n",
       "        0.        , 0.        , 0.35355339, 0.        , 0.        ,\n",
       "        0.        , 0.35355339, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.4472136 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.4472136 , 0.        , 0.4472136 , 0.4472136 ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.25277267, 0.25277267, 0.25277267, 0.        , 0.        ,\n",
       "        0.25277267, 0.2039354 , 0.        , 0.        , 0.        ,\n",
       "        0.25277267, 0.        , 0.        , 0.50554534, 0.        ,\n",
       "        0.25277267, 0.25277267, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25277267,\n",
       "        0.25277267, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25277267, 0.        , 0.        , 0.25277267],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.4695148 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4695148 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.5819515 ,\n",
       "        0.        , 0.        , 0.4695148 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.39835162, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.39835162, 0.        ,\n",
       "        0.        , 0.39835162, 0.32138758, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.39835162, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32138758, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.39835162, 0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method to generate tfidf vector for every token\n",
    "def generate_tfidf_vector(token_set):\n",
    "    text_corpus = []\n",
    "    for token_list in token_set:\n",
    "        text_corpus.append(''.join(x + ' ' for x in token_list))\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(text_corpus)\n",
    "    return tfidf_vectors.toarray()\n",
    "\n",
    "#Selecting 5 random tweets for tfidf vectorizer\n",
    "random_tweets = np.random.choice(tweets['text'], 5)\n",
    "result = []\n",
    "for tweet in random_tweets:\n",
    "    tokenized = tokenize_words(tweet)\n",
    "    usernames, urls, emojis = get_patterns_from_tokens(tokenized)\n",
    "    removed_username_tokenized = remove_usernames_from_tokens(tokenized)\n",
    "    lower_tokenized = adjust_lowercase(removed_username_tokenized)\n",
    "    punctuated_tokenized = remove_punctuations(lower_tokenized)\n",
    "    number_tokenized = remove_numbers(punctuated_tokenized)\n",
    "    stopword_tokenized = remove_stopwords(punctuated_tokenized)\n",
    "    lemma_tokenized = generate_lemmas(stopword_tokenized)\n",
    "    autocorrected_tokenized = autocorrect_tokens(lemma_tokenized)\n",
    "    result.append(autocorrected_tokenized)\n",
    "    \n",
    "#Obtaining the tfidf vector for the five vectors\n",
    "generate_tfidf_vector(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df147a0",
   "metadata": {},
   "source": [
    "There are many different types of NLP processing techniques out there. And even with the ones applied, I haven't displayed how much they influence the result in case of this dataset. It might be the case that the results are better for not applying any of these and it might be the other way around as well, it all depends on the type of your problem and what kind of values are present in the dataset. Thats it for now! This section for covering NLP preprocessing technqiues was done so that we can look into coding approaches for new set of problems. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
